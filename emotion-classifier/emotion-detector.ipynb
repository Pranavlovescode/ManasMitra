{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fffca5e-d3cd-4e7d-b75c-15738b2da803",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03100b16-b416-496a-8261-790b88093d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pranavtitambe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/pranavtitambe/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f5c40f6-1448-455f-99c4-9dae4d6d51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b69e5-1e55-4886-b91d-7dcfb88be627",
   "metadata": {},
   "source": [
    "### Checking GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e9c742-7f03-46a9-83ed-da7f4b63b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 12 16:32:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8              3W /   30W |     380MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2906      G   /usr/bin/gnome-shell                      1MiB |\n",
      "|    0   N/A  N/A            8867      C   ...emotion-classifier/bin/python        360MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c332dcc-63f1-4ef7-8388-fce6cf359a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0438834-0e7d-46e7-b9bb-2960b229ef9e",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad85ea5-904e-4fd3-860d-6aace6a0332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "texts = dataset[\"train\"][\"text\"]\n",
    "labels = dataset[\"train\"][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b2bfea3-1467-44ee-914e-b545d9ac2574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1200001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "glove_path='./dolma_300_2024_1.2M.100_combined.txt'\n",
    "# Load embeddings into a dictionary\n",
    "glove_model = {}\n",
    "with open(glove_path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_model[word] = vector\n",
    "\n",
    "print(f\"Loaded {len(glove_model)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934ba23-027d-479d-82f3-7e78d49b997e",
   "metadata": {},
   "source": [
    "### Label preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d8debb3-562c-4ab1-8a68-cd269a1eaf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 28\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(labels)\n",
    "num_classes = len(mlb.classes_)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7c29a-9021-42e4-aa74-b3064dbf05e5",
   "metadata": {},
   "source": [
    "### Tokenization + Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f6cd3f-4384-47f1-9930-37794795ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(t.lower()) for t in texts]\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = [word for sent in tokenized for word in sent]\n",
    "word_freq = Counter(all_words)\n",
    "vocab = {w: i + 2 for i, (w, _) in enumerate(word_freq.most_common(20000))}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "\n",
    "def encode_text(tokens, max_len=40):\n",
    "    ids = [vocab.get(t, 1) for t in tokens[:max_len]]\n",
    "    ids += [0] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "encoded_texts = [encode_text(t) for t in tokenized]\n",
    "X = torch.tensor(encoded_texts)\n",
    "Y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29097f61-2227-4b4d-8509-49df2106bdad",
   "metadata": {},
   "source": [
    "### Dataset + Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae0b3b54-8e3a-47a5-afc2-e454058674df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(EmotionDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(EmotionDataset(X_val, y_val), batch_size=64)\n",
    "\n",
    "\n",
    "embedding_dim = 300\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize embedding matrix with zeros\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Fill matrix with GloVe vectors\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[idx] = glove_model[word]\n",
    "    else:\n",
    "        # If word not in GloVe, leave it as zeros (or random)\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ea62f4-d356-4c9e-80d5-b12cb567c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4e76e-3fe3-4eed-b763-b5a8b27386cc",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e37fb80-6a4b-4691-bdb0-78b3a5c053ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEmotionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, embedding_matrix):\n",
    "        super().__init__()\n",
    "        # Load pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32), \n",
    "            freeze=False  # set True to freeze embeddings\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(emb)\n",
    "        # Concatenate final states from both directions\n",
    "        out = self.fc(torch.cat((h_n[-2], h_n[-1]), dim=1))\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6fbe1-4ebf-4a7c-a0ef-b1593bf0693f",
   "metadata": {},
   "source": [
    "### Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e4ac277-b6b1-4743-baa7-034233eee83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "embedding_dim = 300  # must match glove embeddings\n",
    "hidden_dim = 256\n",
    "\n",
    "model = LSTMEmotionClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def multi_label_accuracy(y_pred, y_true):\n",
    "    preds = (y_pred > 0.5).float()\n",
    "    correct = (preds == y_true).float().sum()\n",
    "    total = y_true.numel()\n",
    "    return (correct / total).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d12b1c-6be8-4e2b-bf91-fa142dabfb40",
   "metadata": {},
   "source": [
    "### Training Loop with epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "921b4404-ce5c-43c7-86b1-1612d1af0cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15]\n",
      "Train Loss: 0.0161, Train Acc: 99.47%\n",
      "Val Loss:   0.2006, Val Acc:   95.93%\n",
      "------------------------------------------------------------\n",
      "Epoch [2/15]\n",
      "Train Loss: 0.0137, Train Acc: 99.55%\n",
      "Val Loss:   0.2267, Val Acc:   95.84%\n",
      "------------------------------------------------------------\n",
      "Epoch [3/15]\n",
      "Train Loss: 0.0117, Train Acc: 99.62%\n",
      "Val Loss:   0.2311, Val Acc:   95.82%\n",
      "------------------------------------------------------------\n",
      "Epoch [4/15]\n",
      "Train Loss: 0.0101, Train Acc: 99.68%\n",
      "Val Loss:   0.2468, Val Acc:   95.88%\n",
      "------------------------------------------------------------\n",
      "Epoch [5/15]\n",
      "Train Loss: 0.0087, Train Acc: 99.72%\n",
      "Val Loss:   0.2640, Val Acc:   95.83%\n",
      "------------------------------------------------------------\n",
      "Epoch [6/15]\n",
      "Train Loss: 0.0077, Train Acc: 99.75%\n",
      "Val Loss:   0.2732, Val Acc:   95.82%\n",
      "------------------------------------------------------------\n",
      "Epoch [7/15]\n",
      "Train Loss: 0.0066, Train Acc: 99.80%\n",
      "Val Loss:   0.3027, Val Acc:   95.76%\n",
      "------------------------------------------------------------\n",
      "Epoch [8/15]\n",
      "Train Loss: 0.0059, Train Acc: 99.82%\n",
      "Val Loss:   0.3113, Val Acc:   95.71%\n",
      "------------------------------------------------------------\n",
      "Epoch [9/15]\n",
      "Train Loss: 0.0054, Train Acc: 99.84%\n",
      "Val Loss:   0.3290, Val Acc:   95.72%\n",
      "------------------------------------------------------------\n",
      "Epoch [10/15]\n",
      "Train Loss: 0.0047, Train Acc: 99.86%\n",
      "Val Loss:   0.3230, Val Acc:   95.75%\n",
      "------------------------------------------------------------\n",
      "Epoch [11/15]\n",
      "Train Loss: 0.0040, Train Acc: 99.89%\n",
      "Val Loss:   0.3421, Val Acc:   95.71%\n",
      "------------------------------------------------------------\n",
      "Epoch [12/15]\n",
      "Train Loss: 0.0041, Train Acc: 99.88%\n",
      "Val Loss:   0.3553, Val Acc:   95.74%\n",
      "------------------------------------------------------------\n",
      "Epoch [13/15]\n",
      "Train Loss: 0.0034, Train Acc: 99.90%\n",
      "Val Loss:   0.3512, Val Acc:   95.66%\n",
      "------------------------------------------------------------\n",
      "Epoch [14/15]\n",
      "Train Loss: 0.0030, Train Acc: 99.92%\n",
      "Val Loss:   0.3691, Val Acc:   95.78%\n",
      "------------------------------------------------------------\n",
      "Epoch [15/15]\n",
      "Train Loss: 0.0034, Train Acc: 99.90%\n",
      "Val Loss:   0.3750, Val Acc:   95.74%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += multi_label_accuracy(outputs, y_batch)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_train_acc = total_acc / len(train_loader)\n",
    "\n",
    "    # ===========================\n",
    "    # Validation\n",
    "    # ===========================\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += multi_label_accuracy(outputs, y_batch)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_acc = val_acc / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss:   {avg_val_loss:.4f}, Val Acc:   {avg_val_acc*100:.2f}%\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c90aeda1-967d-4e49-a292-7ba2c5ff76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# def predict_emotions(text, tokenizer, model, device, label_names):\n",
    "#     # Tokenize single text\n",
    "#     encoding = tokenizer(\n",
    "#         text,\n",
    "#         return_tensors='pt',\n",
    "#         truncation=True,\n",
    "#         padding='max_length',\n",
    "#         max_length=128\n",
    "#     )\n",
    "\n",
    "#     # Move to GPU/CPU\n",
    "#     input_ids = encoding['input_ids'].to(device)\n",
    "#     attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "#     # Run through model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, attention_mask)\n",
    "#         probs = sigmoid(outputs).cpu().numpy()[0]\n",
    "\n",
    "#     # Convert probabilities to label names (threshold = 0.5)\n",
    "#     threshold = 0.5\n",
    "#     predicted_labels = [label_names[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "#     return predicted_labels, probs\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ensure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# ðŸ”¹ Function to encode and predict emotions for a single text\n",
    "def predict_emotion_lstm(text, model, vocab, device, label_names):\n",
    "    # Tokenize + numericalize\n",
    "    tokens = [vocab.get(word.lower(), vocab['<UNK>']) for word in text.split()]\n",
    "    # Pad/truncate\n",
    "    max_len = 50\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [vocab['<PAD>']] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "\n",
    "    # Convert to tensor\n",
    "    x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        probs = model(x).cpu().numpy()[0]\n",
    "\n",
    "    # Apply threshold\n",
    "    threshold = 0.5\n",
    "    predicted_labels = [label_names[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "\n",
    "    return predicted_labels, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86c60811-44d6-416f-9463-29f3f669f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\",\n",
    "    \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\",\n",
    "    \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\",\n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\",\n",
    "    \"surprise\", \"neutral\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5334d1f1-78cd-4c17-9edc-3eec038578d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I am so happy and grateful for your help!\n",
      "Predicted Emotions: ['gratitude', 'joy']\n",
      "\n",
      "Text: This is the worst movie Iâ€™ve ever seen.\n",
      "Predicted Emotions: ['disgust']\n",
      "\n",
      "Text: I canâ€™t believe I actually won the competition!\n",
      "Predicted Emotions: ['love']\n",
      "\n",
      "Text: I feel nervous about my job interview tomorrow.\n",
      "Predicted Emotions: ['nervousness']\n",
      "\n",
      "Text: Wow, that performance was absolutely amazing!\n",
      "Predicted Emotions: ['admiration']\n",
      "\n",
      "Text: Iâ€™m really disappointed with your behavior.\n",
      "Predicted Emotions: ['disappointment']\n",
      "\n",
      "Text: I miss you so much it hurts.\n",
      "Predicted Emotions: ['sadness']\n",
      "\n",
      "Text: What a confusing tutorial... I donâ€™t get it.\n",
      "Predicted Emotions: ['confusion']\n",
      "\n",
      "Text: Iâ€™m proud of what Iâ€™ve achieved today.\n",
      "Predicted Emotions: ['admiration']\n",
      "\n",
      "Text: Nothing special happened today, just a normal day.\n",
      "Predicted Emotions: ['neutral']\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"I am so happy and grateful for your help!\",\n",
    "    \"This is the worst movie Iâ€™ve ever seen.\",\n",
    "    \"I canâ€™t believe I actually won the competition!\",\n",
    "    \"I feel nervous about my job interview tomorrow.\",\n",
    "    \"Wow, that performance was absolutely amazing!\",\n",
    "    \"Iâ€™m really disappointed with your behavior.\",\n",
    "    \"I miss you so much it hurts.\",\n",
    "    \"What a confusing tutorial... I donâ€™t get it.\",\n",
    "    \"Iâ€™m proud of what Iâ€™ve achieved today.\",\n",
    "    \"Nothing special happened today, just a normal day.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    labels, probs = predict_emotion_lstm(text, model, vocab, device, label_names)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted Emotions: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa2122-0fa6-4b90-850b-806e3b3767e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
